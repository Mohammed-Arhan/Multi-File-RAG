{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "llama-index\n",
        "google-generativeai\n",
        "llama-index-llms-gemini\n",
        "pypdf\n",
        "python-dotenv\n",
        "IPython\n",
        "llama-index-embeddings-gemini\n",
        "streamlit\n",
        "\n",
        "-e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdortI52lG8F",
        "outputId": "3c7353a2-7e35-48ce-a82b-f3835235b710"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "_O8PuWf_lShb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "1uTJY3xQd22K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.core import ServiceContext\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from IPython.display import Markdown, display\n",
        "import os\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Settings"
      ],
      "metadata": {
        "id": "p-snenQDlhoO"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "for models in genai.list_models():\n",
        "  if \"generateContent\" in models.supported_generation_methods:\n",
        "    print(models.name)\n",
        "\n",
        "#Use generateContent for text to text and image to text"
      ],
      "metadata": {
        "id": "XJNGWR9amWFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for models in genai.list_models():\n",
        "  print(models)"
      ],
      "metadata": {
        "id": "5TP3TUpu3Xxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **END-TO-END**"
      ],
      "metadata": {
        "id": "U0Gc-BsN8xoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile exception.py\n",
        "import sys\n",
        "\n",
        "\n",
        "class customexception(Exception):\n",
        "\n",
        "    def __init__(self,error_message,error_details:sys):\n",
        "        self.error_message=error_message\n",
        "        _,_,exc_tb=error_details.exc_info()\n",
        "        print(exc_tb)\n",
        "\n",
        "        self.lineno=exc_tb.tb_lineno\n",
        "        self.file_name=exc_tb.tb_frame.f_code.co_filename\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Error occured in python script name [{0}] line number [{1}] error message [{2}]\".format(\n",
        "        self.file_name, self.lineno, str(self.error_message))\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    try:\n",
        "        a=1/0\n",
        "\n",
        "    except Exception as e:\n",
        "        #print(e)\n",
        "        raise customexception(e,sys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ala0p_c48b7L",
        "outputId": "d78145b8-1d91-410a-b245-e319f1c91211"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting exception.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile logger.py\n",
        "import logging\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "LOG_FILE=f\"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log\"\n",
        "\n",
        "log_path=os.path.join(os.getcwd(),\"logs\")\n",
        "\n",
        "os.makedirs(log_path,exist_ok=True)\n",
        "\n",
        "LOG_FILEPATH=os.path.join(log_path,LOG_FILE)\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    filename=LOG_FILEPATH,\n",
        "                    format=\"[%(asctime)s] %(lineno)d %(name)s - %(levelname)s - %(message)s\"\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZnuSfa4Lirs",
        "outputId": "0bab4960-761c-4758-e752-1ea23454a3a2"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting logger.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile setup.py\n",
        "from setuptools import find_packages, setup\n",
        "\n",
        "setup(\n",
        "    name = 'QApplication',\n",
        "    version= '0.0.1',\n",
        "    author= 'sunny savita',\n",
        "    author_email= 'sunny.savita@gmail.com',\n",
        "    packages= find_packages(),\n",
        "    install_requires = []\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2O4OWXyMShO",
        "outputId": "8ae8ad21-c16e-4560-83b8-5ab5f1556186"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile template.py\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "list_of_files=[\n",
        "    \"QAWithPDF/__init__.py\",\n",
        "    \"QAWithPDF/data_ingestion.py\",\n",
        "    \"QAWithPDF/embedding.py\",\n",
        "    \"QAWithPDF/model_api.py\",\n",
        "    \"Experiments/experiment.ipynb\",\n",
        "    \"StreamlitApp.py\",\n",
        "    \"logger.py\",\n",
        "    \"exception.py\",\n",
        "    \"setup.py\"\n",
        "        ]\n",
        "\n",
        "\n",
        "for filepath in list_of_files:\n",
        "   filepath = Path(filepath)\n",
        "   filedir, filename = os.path.split(filepath)\n",
        "\n",
        "   if filedir !=\"\":\n",
        "      os.makedirs(filedir, exist_ok=True)\n",
        "\n",
        "   if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\n",
        "      with open(filepath, 'w') as f:\n",
        "         pass"
      ],
      "metadata": {
        "id": "Jm5Ze_YBMXvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd0cb0d1-d48e-436d-e690-38b786c03dc0"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing template.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_ingestion.py\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "import sys\n",
        "from exception import customexception\n",
        "from logger import logging\n",
        "\n",
        "def load_data(data):\n",
        "    \"\"\"\n",
        "    Load PDF documents from a specified directory.\n",
        "\n",
        "    Parameters:\n",
        "    - data (str): The path to the directory containing PDF files.\n",
        "\n",
        "    Returns:\n",
        "    - A list of loaded PDF documents. The specific type of documents may vary.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"data loading started...\")\n",
        "        loader = SimpleDirectoryReader(\"Data\")\n",
        "        documents=loader.load_data()\n",
        "        logging.info(\"data loading completed...\")\n",
        "        return documents\n",
        "    except Exception as e:\n",
        "        logging.info(\"exception in loading data...\")\n",
        "        raise customexception(e,sys)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5rH7TeXP7Th",
        "outputId": "c24e3508-793e-4f74-cc11-7fa9fba55d1e"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_ingestion.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile embedding.py\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core import ServiceContext\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from data_ingestion import load_data\n",
        "from model_api import load_model\n",
        "\n",
        "import sys\n",
        "from exception import customexception\n",
        "from logger import logging\n",
        "\n",
        "def download_gemini_embedding(model,document):\n",
        "    \"\"\"\n",
        "    Downloads and initializes a Gemini Embedding model for vector embeddings.\n",
        "\n",
        "    Returns:\n",
        "    - VectorStoreIndex: An index of vector embeddings for efficient similarity queries.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"\")\n",
        "        gemini_embed_model = GeminiEmbedding(model_name=\"models/embedding-001\")\n",
        "        node_parser = SentenceSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "        transformations = [node_parser]\n",
        "\n",
        "        logging.info(\"\")\n",
        "        index = VectorStoreIndex.from_documents(documents=document, embed_model=gemini_embed_model, transformations=transformations)\n",
        "\n",
        "        index.storage_context.persist()\n",
        "\n",
        "        logging.info(\"\")\n",
        "        query_engine = index.as_query_engine(llm=model)\n",
        "        return query_engine\n",
        "    except Exception as e:\n",
        "        raise customexception(e,sys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy25Kmm6QAM3",
        "outputId": "d4f498f1-6f71-4e35-89e1-9c93463dceef"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting embedding.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_api.py\n",
        "\n",
        "import os\n",
        "\n",
        "import sys\n",
        "\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from IPython.display import Markdown, display\n",
        "import google.generativeai as genai\n",
        "from exception import customexception\n",
        "from logger import logging\n",
        "\n",
        "\n",
        "GOOGLE_API_KEY=os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "def load_model():\n",
        "\n",
        "    \"\"\"\n",
        "    Loads a Gemini-Pro model for natural language processing.\n",
        "\n",
        "    Returns:\n",
        "    - Gemini: An instance of the Gemini class initialized with the 'gemini-pro' model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model=Gemini(models='gemini-pro',api_key=GOOGLE_API_KEY)\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        raise customexception(e,sys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQkA21UxQN2j",
        "outputId": "0a0cf066-bece-465e-fb0b-a68c285e80f1"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model_api.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from data_ingestion import load_data\n",
        "from embedding import download_gemini_embedding\n",
        "from model_api import load_model\n",
        "\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(\"QA with Documents\")\n",
        "\n",
        "    doc=st.file_uploader(\"upload your document\")\n",
        "\n",
        "    st.header(\"QA with Documents(Information Retrieval)\")\n",
        "\n",
        "    user_question= st.text_input(\"Ask your question\")\n",
        "\n",
        "    if st.button(\"submit & process\"):\n",
        "        with st.spinner(\"Processing...\"):\n",
        "            document=load_data(doc)\n",
        "            model=load_model()\n",
        "            query_engine=download_gemini_embedding(model,document)\n",
        "\n",
        "            response = query_engine.query(user_question)\n",
        "\n",
        "            st.write(response.response)\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IueAJpenPOc_",
        "outputId": "e738b22b-fc4c-4940-84e6-c3637e9b0306"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl icanhazip.com\n",
        "print()\n",
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISPml3q9d6R8",
        "outputId": "2f97c905-c518-4f90-83a6-160c438bf3c9"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.229.220.181\n",
            "\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.229.220.181:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0Kyour url is: https://poor-shrimps-bet.loca.lt\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1740408923.298370   42155 init.cc:232] grpc_wait_for_shutdown_with_timeout() timed out.\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iTaMPP-XQX1S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}